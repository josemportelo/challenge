{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jmlp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jmlp/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmlp/JMLP/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "# -------\n",
    "#import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#%matplotlib inline\n",
    "\n",
    "#import string\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    29720\n",
      "1     2242\n",
      "Name: label, dtype: int64\n",
      "   = TRAIN =\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31962 entries, 0 to 31961\n",
      "Data columns (total 3 columns):\n",
      "id       31962 non-null int64\n",
      "label    31962 non-null int64\n",
      "tweet    31962 non-null object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 749.2+ KB\n",
      "None\n",
      "(31962, 3)\n",
      "   id  label                                              tweet\n",
      "0   1      0   @user when a father is dysfunctional and is s...\n",
      "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
      "2   3      0                                bihday your majesty\n",
      "3   4      0  #model   i love u take with u all the time in ...\n",
      "4   5      0             factsguide: society now    #motivation\n",
      "   = TEST =\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17197 entries, 0 to 17196\n",
      "Data columns (total 2 columns):\n",
      "id       17197 non-null int64\n",
      "tweet    17197 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 268.8+ KB\n",
      "None\n",
      "(17197, 2)\n",
      "          id                                              tweet\n",
      "17192  49155  thought factory: left-right polarisation! #tru...\n",
      "17193  49156  feeling like a mermaid ð #hairflip #neverre...\n",
      "17194  49157  #hillary #campaigned today in #ohio((omg)) &am...\n",
      "17195  49158  happy, at work conference: right mindset leads...\n",
      "17196  49159  my   song \"so glad\" free download!  #shoegaze ...\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "# ---------\n",
    "df_train_BASE  = pd.read_csv('train_E6oV3lV.csv')\n",
    "df_test_BASE = pd.read_csv('test_tweets_anuFYb8.csv')\n",
    "\n",
    "# check training labels\n",
    "print df_train_BASE['label'].value_counts()\n",
    "\n",
    "# ### CHECKPOINT START ###\n",
    "print(\"   = TRAIN =\")\n",
    "print df_train_BASE.info()\n",
    "print df_train_BASE.shape\n",
    "print(df_train_BASE.head())\n",
    "print(\"   = TEST =\")\n",
    "print df_test_BASE.info()\n",
    "print df_test_BASE.shape\n",
    "print(df_test_BASE.tail())\n",
    "# ### CHECKPOINT STOP ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   = before data cleaning =\n",
      "   id  label                                              tweet\n",
      "0   1    0.0   @user when a father is dysfunctional and is s...\n",
      "1   2    0.0  @user @user thanks for #lyft credit i can't us...\n",
      "2   3    0.0                                bihday your majesty\n",
      "3   4    0.0  #model   i love u take with u all the time in ...\n",
      "4   5    0.0             factsguide: society now    #motivation\n",
      "5   6    0.0  [2/2] huge fan fare and big talking before the...\n",
      "6   7    0.0   @user camping tomorrow @user @user @user @use...\n",
      "7   8    0.0  the next school year is the year for exams.ð...\n",
      "8   9    0.0  we won!!! love the land!!! #allin #cavs #champ...\n",
      "9  10    0.0   @user @user welcome here !  i'm   it's so #gr...\n",
      "   = after data cleaning =\n",
      "   id  label                                              tweet\n",
      "0   1    0.0  father dysfunctional selfish drag kid dysfunction\n",
      "1   2    0.0   thank credit use cause offer wheelchair vans pdx\n",
      "2   3    0.0                                     bihday majesty\n",
      "3   4    0.0                                     love take time\n",
      "4   5    0.0                                 factsguide society\n",
      "5   6    0.0  huge fan fare big talk leave chaos pay dispute...\n",
      "6   7    0.0                                camp tomorrow danny\n",
      "7   8    0.0                  next school year year exams think\n",
      "8   9    0.0                                          love land\n",
      "9  10    0.0                                            welcome\n"
     ]
    }
   ],
   "source": [
    "# combine training and testing sets, for easier data cleaning\n",
    "# -----------------------------------------------------------\n",
    "df_train_test = df_train_BASE.append(df_test_BASE, ignore_index=True)\n",
    "#df_train_test = df_train_BASE.append(df_test_BASE, ignore_index=True).head(200)\n",
    "\n",
    "#print(df_train_test) # sanity check\n",
    "\n",
    "\n",
    "# data cleaning\n",
    "# -------------\n",
    "print(\"   = before data cleaning =\")\n",
    "print(df_train_test.head(10))\n",
    "\n",
    "\n",
    "# remove usernames and hashtags --> this information may be relevant for a deeper analysis, but it will be ignored for now\n",
    "df_train_test['tweet'] = df_train_test['tweet'].str.replace(\"@[A-Za-z0-9_]+\",\"\") # usernames\n",
    "df_train_test['tweet'] = df_train_test['tweet'].str.replace(\"#[A-Za-z0-9_]+\",\"\") # hashtags\n",
    "\n",
    "# remove special characters, numbers, punctuations\n",
    "df_train_test['tweet'] = df_train_test['tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# force lowercase --> reduce the feature space\n",
    "df_train_test['tweet'] = df_train_test['tweet'].apply(lambda x: ' '.join([word.lower() for word in x.split()]))\n",
    "\n",
    "\n",
    "# remove/correct misspelled words (ex: desparately), slang (ex: cuz, u), and others (ex: juuuuuust) --> reduce the feature space\n",
    "# ### NOT ENOUGH TIME TO IMPLEMENT ###\n",
    "\n",
    "# remove short words --> this is not the best approach, as it indiscriminately removes potentially useful words --> remove stopwords instead\n",
    "df_train_test['tweet'] = df_train_test['tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "\n",
    "# remove stopwords, in order to keep only sentiment-relevant words --> this still removes some relevant words (ex: not), meaning that a more careful analysis is required\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "df_train_test['tweet'] = df_train_test['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords_list)]))\n",
    "\n",
    "\n",
    "# perform stemming --> stemmer operates on a single word without knowledge of the context --> perform lemmatization instead\n",
    "#stemmer = PorterStemmer()\n",
    "#df_train_test['tweet'] = df_train_test['tweet'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "\n",
    "# perform lemmatization: group together the inflected forms of a word --> reduce the feature space\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df_train_test['tweet'] = df_train_test['tweet'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word,'v') for word in x.split()]))\n",
    "\n",
    "# perform Part-of-Speech (PoS) tagging --> ### NOT ENOUGH TIME TO CHECK IF IT CAN BE USEFUL ###\n",
    "#df_train_test['PoS_tweet'] = df_train_test['tweet'].apply(lambda x: nltk.pos_tag(nltk.word_tokenize(x)))\n",
    "\n",
    "\n",
    "print(\"   = after data cleaning =\")\n",
    "print(df_train_test.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'able', u'absolutely', u'abt', u'accept', u'account', u'act', u'action', u'actually', u'adapt', u'add', u'adventure', u'affect', u'afternoon', u'age', u'ago', u'agree', u'ahead', u'aicle', u'air', u'album', u'alive', u'alligator', u'allow', u'amaze', u'america', u'american', u'americans', u'amp', u'angry', u'animals', u'anniversary', u'announce', u'answer', u'anti', u'anymore', u'app', u'apparently', u'appreciate', u'arrive', u'ask', u'ass', u'attack', u'attend', u'attention', u'august', u'available', u'award', u'away', u'awesome', u'aww', u'baby', u'bad', u'bag', u'balance', u'ball', u'ban', u'band', u'bank', u'bar', u'base', u'bday', u'beach', u'bear', u'beat', u'beautiful', u'beauty', u'bed', u'beer', u'begin', u'believe', u'benefit', u'best', u'bet', u'better', u'big', u'biggest', u'bihday', u'bike', u'bing', u'bird', u'bitch', u'bite', u'black', u'blame', u'bless', u'block', u'blog', u'blow', u'blue', u'board', u'boat', u'body', u'bong', u'book', u'bore', u'boss', u'bottle', u'box', u'boy', u'boyfriend', u'boys', u'brain', u'brand', u'break', u'breakfast', u'brexit', u'brilliant', u'bring', u'bro', u'broker', u'brother', u'brown', u'buffalo', u'build', u'bull', u'burn', u'business', u'busy', u'buy', u'cake', u'camp', u'campaign', u'cancel', u'car', u'card', u'care', u'case', u'cat', u'catch', u'cause', u'cavs', u'celebrate', u'challenge', u'chance', u'change', u'channel', u'charge', u'chase', u'check', u'cheer', u'child', u'children', u'chill', u'choice', u'choose', u'christina', u'christmas', u'church', u'city', u'claim', u'class', u'clean', u'clear', u'clearly', u'click', u'client', u'climb', u'close', u'clothe', u'club', u'coach', u'cock', u'coffee', u'cold', u'college', u'color', u'come', u'comment', u'community', u'company', u'complete', u'completely', u'conce', u'condemn', u'conference', u'confirm', u'congrats', u'congratulations', u'connect', u'content', u'continue', u'control', u'cook', u'cool', u'count', u'countdown', u'country', u'couple', u'course', u'cover', u'cox', u'crazy', u'cream', u'create', u'crime', u'cross', u'culture', u'cup', u'current', u'customer', u'customers', u'cut', u'cute', u'cuz', u'dad', u'daddy', u'dads', u'daily', u'damn', u'dance', u'dark', u'date', u'daughter', u'day', u'days', u'dead', u'deal', u'dear', u'death', u'decide', u'decision', u'deep', u'definitely', u'delete', u'deliver', u'deserve', u'design', u'despite', u'destroy', u'development', u'die', u'diet', u'different', u'dinner', u'direct', u'disappoint', u'disgust', u'disney', u'dog', u'dominate', u'donald', u'dont', u'door', u'dory', u'double', u'download', u'draw', u'dream', u'dress', u'drink', u'drive', u'drop', u'dude', u'eah', u'early', u'easy', u'eat', u'election', u'email', u'end', u'england', u'english', u'enjoy', u'entire', u'environment', u'episode', u'especially', u'euro', u'event', u'events', u'everybody', u'everyday', u'exactly', u'excellent', u'excite', u'expanse', u'expect', u'experience', u'eye', u'fab', u'face', u'facebook', u'fact', u'factory', u'fail', u'fake', u'fall', u'families', u'family', u'fan', u'fantastic', u'far', u'fast', u'father', u'fathersday', u'favorite', u'favourite', u'fear', u'feed', u'feel', u'feet', u'felt', u'female', u'festival', u'fight', u'film', u'final', u'finally', u'fine', u'finger', u'finish', u'fit', u'fix', u'flag', u'flight', u'florida', u'flower', u'fly', u'focus', u'folks', u'follow', u'followers', u'food', u'football', u'force', u'forever', u'forget', u'forward', u'france', u'free', u'freedom', u'fresh', u'friday', u'friend', u'friends', u'fuck', u'fun', u'funny', u'future', u'gain', u'game', u'garden', u'gay', u'gbp', u'generation', u'germany', u'gift', u'girl', u'girls', u'glad', u'glass', u'goal', u'god', u'gonna', u'good', u'google', u'gop', u'gorgeous', u'gorilla', u'gotta', u'graduate', u'grateful', u'great', u'greatest', u'green', u'grimmie', u'group', u'grow', u'guess', u'gun', u'guy', u'gym', u'haha', u'hair', u'half', u'hand', u'hang', u'happen', u'happier', u'happiest', u'happiness', u'happy', u'hard', u'hardcore', u'hat', u'hate', u'hatred', u'hea', u'head', u'heal', u'health', u'healthy', u'hear', u'heas', u'hell', u'hello', u'help', u'hero', u'hey', u'high', u'hill', u'hillary', u'history', u'hit', u'hold', u'holiday', u'home', u'honor', u'hop', u'hope', u'hopefully', u'horrible', u'host', u'hot', u'hotel', u'hour', u'hours', u'house', u'hrs', u'htt', u'hug', u'huge', u'human', u'humanity', u'humans', u'husband', u'ice', u'idea', u'ignorance', u'ignore', u'image', u'imagine', u'impoant', u'include', u'india', u'info', u'injure', u'innocent', u'inside', u'inspire', u'instagram', u'instead', u'internet', u'interview', u'involve', u'islam', u'issue', u'jam', u'job', u'john', u'join', u'joke', u'journey', u'joy', u'jpy', u'judge', u'july', u'june', u'key', u'kick', u'kid', u'kill', u'kind', u'king', u'kiss', u'know', u'lack', u'ladies', u'lady', u'lake', u'land', u'late', u'later', u'latest', u'laugh', u'launch', u'law', u'lead', u'leakage', u'learn', u'leave', u'lebron', u'let', u'level', u'libtard', u'lie', u'life', u'light', u'like', u'line', u'link', u'list', u'listen', u'literally', u'little', u'live', u'local', u'logins', u'lol', u'london', u'long', u'longer', u'look', u'lord', u'lose', u'loss', u'lot', u'love', u'lovely', u'lover', u'low', u'luck', u'lucky', u'lunch', u'lyric', u'mad', u'magic', u'main', u'make', u'man', u'mark', u'market', u'marry', u'mass', u'match', u'material', u'matter', u'maybe', u'mean', u'media', u'meet', u'member', u'members', u'memories', u'men', u'mention', u'message', u'michael', u'middle', u'million', u'mind', u'mindset', u'mins', u'minute', u'minutes', u'miss', u'mix', u'mom', u'moment', u'moments', u'monday', u'money', u'month', u'months', u'mood', u'moon', u'morning', u'mother', u'mountains', u'movie', u'movies', u'murder', u'music', u'muslim', u'muslims', u'nail', u'naked', u'nation', u'national', u'nba', u'near', u'nearly', u'need', u'negative', u'ness', u'new', u'news', u'nice', u'niggas', u'night', u'nights', u'non', u'note', u'notice', u'nude', u'number', u'obama', u'offer', u'office', u'officer', u'official', u'officially', u'oil', u'okay', u'old', u'omg', u'ones', u'online', u'open', u'orange', u'order', u'organizations', u'orlando', u'outside', u'pack', u'page', u'pain', u'paint', u'parent', u'park', u'pass', u'past', u'pathetic', u'pay', u'peace', u'people', u'perfect', u'perform', u'performance', u'person', u'phone', u'photo', u'pic', u'pick', u'pics', u'picture', u'piece', u'pig', u'pizza', u'place', u'plan', u'play', u'pls', u'plus', u'poetry', u'point', u'polar', u'police', u'political', u'politics', u'poor', u'pop', u'porn', u'positive', u'possible', u'post', u'power', u'powerful', u'ppl', u'practice', u'pray', u'prayers', u'pre', u'prepare', u'present', u'president', u'press', u'pretty', u'previous', u'price', u'pride', u'print', u'probably', u'problem', u'problems', u'product', u'profile', u'project', u'promote', u'protest', u'proud', u'prove', u'public', u'purchase', u'pussy', u'queen', u'question', u'quick', u'quote', u'race', u'racism', u'racist', u'rain', u'raise', u'rally', u'ramadan', u'rat', u'reach', u'read', u'ready', u'real', u'reality', u'realize', u'really', u'reason', u'receive', u'record', u'red', u'register', u'relax', u'release', u'religion', u'remain', u'remember', u'remind', u'respect', u'response', u'rest', u'result', u'return', u'retweet', u'reveal', u'review', u'rid', u'ride', u'right', u'rip', u'rise', u'risk', u'road', u'robe', u'rock', u'role', u'roll', u'room', u'rooster', u'round', u'route', u'row', u'rule', u'run', u'sad', u'sadly', u'safe', u'sale', u'saturday', u'save', u'saw', u'say', u'scar', u'school', u'screen', u'sea', u'search', u'season', u'seat', u'second', u'secret', u'seek', u'self', u'selfie', u'sell', u'send', u'sense', u'senseless', u'series', u'seriously', u'serve', u'service', u'session', u'set', u'sex', u'sexy', u'shame', u'share', u'shi', u'shift', u'shit', u'sho', u'shock', u'shoe', u'shoot', u'shop', u'shout', u'shut', u'sick', u'sign', u'simple', u'simulation', u'simulator', u'sing', u'single', u'sister', u'sit', u'size', u'skin', u'sky', u'sleep', u'slow', u'small', u'smh', u'smile', u'snapchat', u'social', u'somebody', u'son', u'song', u'songs', u'soon', u'sorry', u'soul', u'sound', u'source', u'south', u'space', u'speak', u'special', u'speech', u'spend', u'spos', u'spread', u'sta', u'staed', u'staff', u'stage', u'staing', u'stand', u'star', u'stas', u'state', u'stay', u'steal', u'step', u'stick', u'stock', u'stomp', u'stop', u'store', u'stories', u'story', u'straight', u'stream', u'street', u'strong', u'students', u'study', u'stuff', u'stupid', u'style', u'success', u'suck', u'suffer', u'summer', u'sun', u'sunday', u'sunny', u'sunshine', u'super', u'suppo', u'suppoers', u'suppoing', u'suppose', u'sure', u'surprise', u'survive', u'sweet', u'swim', u'tag', u'talk', u'target', u'task', u'tea', u'teach', u'team', u'tear', u'teen', u'tell', u'term', u'terrible', u'terrorist', u'test', u'thank', u'thankful', u'thats', u'thing', u'things', u'think', u'tho', u'thoughts', u'throw', u'thursday', u'ticket', u'til', u'till', u'time', u'tip', u'tire', u'today', u'tomorrow', u'tonight', u'tool', u'totally', u'touch', u'tour', u'town', u'track', u'tragedy', u'tragic', u'train', u'travel', u'treat', u'tree', u'trend', u'trip', u'troll', u'true', u'truly', u'trump', u'trust', u'truth', u'try', u'tuesday', u'tune', u'turn', u'tweet', u'twitter', u'type', u'understand', u'unite', u'university', u'update', u'usa', u'usd', u'use', u'useful', u'vacation', u'value', u'vast', u'vegas', u'vibes', u'vicinity', u'victims', u'video', u'videos', u'view', u'vine', u'violence', u'visit', u'voice', u'vote', u'wait', u'wake', u'wakeup', u'walk', u'wall', u'wanna', u'want', u'war', u'warm', u'waste', u'watch', u'water', u'way', u'ways', u'wear', u'weather', u'web', u'website', u'wed', u'wednesday', u'week', u'weekend', u'weeks', u'welcome', u'west', u'white', u'wife', u'wild', u'win', u'wine', u'wing', u'winner', u'wish', u'woh', u'woman', u'women', u'wonder', u'wonderful', u'word', u'work', u'world', u'worry', u'worse', u'worst', u'wow', u'write', u'wrong', u'wtf', u'xxx', u'yay', u'yeah', u'year', u'years', u'yes', u'yesterday', u'york', u'young', u'youth', u'youtube', u'yrs']\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Non-zero elements: 145459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'able', u'absolutely', u'abt', u'accept', u'account', u'act', u'action', u'actually', u'adapt', u'add', u'adventure', u'affect', u'afternoon', u'age', u'ago', u'agree', u'ahead', u'aicle', u'air', u'album', u'alive', u'alligator', u'allow', u'amaze', u'america', u'american', u'americans', u'amp', u'angry', u'animals', u'anniversary', u'announce', u'answer', u'anti', u'anymore', u'app', u'apparently', u'appreciate', u'arrive', u'ask', u'ass', u'attack', u'attend', u'attention', u'august', u'available', u'award', u'away', u'awesome', u'aww', u'baby', u'bad', u'bag', u'balance', u'ball', u'ban', u'band', u'bank', u'bar', u'base', u'bday', u'beach', u'bear', u'beat', u'beautiful', u'beauty', u'bed', u'beer', u'begin', u'believe', u'benefit', u'best', u'bet', u'better', u'big', u'biggest', u'bihday', u'bike', u'bing', u'bird', u'bitch', u'bite', u'black', u'blame', u'bless', u'block', u'blog', u'blow', u'blue', u'board', u'boat', u'body', u'bong', u'book', u'bore', u'boss', u'bottle', u'box', u'boy', u'boyfriend', u'boys', u'brain', u'brand', u'break', u'breakfast', u'brexit', u'brilliant', u'bring', u'bro', u'broker', u'brother', u'brown', u'buffalo', u'build', u'bull', u'burn', u'business', u'busy', u'buy', u'cake', u'camp', u'campaign', u'cancel', u'car', u'card', u'care', u'case', u'cat', u'catch', u'cause', u'cavs', u'celebrate', u'challenge', u'chance', u'change', u'channel', u'charge', u'chase', u'check', u'cheer', u'child', u'children', u'chill', u'choice', u'choose', u'christina', u'christmas', u'church', u'city', u'claim', u'class', u'clean', u'clear', u'clearly', u'click', u'client', u'climb', u'close', u'clothe', u'club', u'coach', u'cock', u'coffee', u'cold', u'college', u'color', u'come', u'comment', u'community', u'company', u'complete', u'completely', u'conce', u'condemn', u'conference', u'confirm', u'congrats', u'congratulations', u'connect', u'content', u'continue', u'control', u'cook', u'cool', u'count', u'countdown', u'country', u'couple', u'course', u'cover', u'cox', u'crazy', u'cream', u'create', u'crime', u'cross', u'culture', u'cup', u'current', u'customer', u'customers', u'cut', u'cute', u'cuz', u'dad', u'daddy', u'dads', u'daily', u'damn', u'dance', u'dark', u'date', u'daughter', u'day', u'days', u'dead', u'deal', u'dear', u'death', u'decide', u'decision', u'deep', u'definitely', u'delete', u'deliver', u'deserve', u'design', u'despite', u'destroy', u'development', u'die', u'diet', u'different', u'dinner', u'direct', u'disappoint', u'disgust', u'disney', u'dog', u'dominate', u'donald', u'dont', u'door', u'dory', u'double', u'download', u'draw', u'dream', u'dress', u'drink', u'drive', u'drop', u'dude', u'eah', u'early', u'easy', u'eat', u'election', u'email', u'end', u'england', u'english', u'enjoy', u'entire', u'environment', u'episode', u'especially', u'euro', u'event', u'events', u'everybody', u'everyday', u'exactly', u'excellent', u'excite', u'expanse', u'expect', u'experience', u'eye', u'fab', u'face', u'facebook', u'fact', u'factory', u'fail', u'fake', u'fall', u'families', u'family', u'fan', u'fantastic', u'far', u'fast', u'father', u'fathersday', u'favorite', u'favourite', u'fear', u'feed', u'feel', u'feet', u'felt', u'female', u'festival', u'fight', u'film', u'final', u'finally', u'fine', u'finger', u'finish', u'fit', u'fix', u'flag', u'flight', u'florida', u'flower', u'fly', u'focus', u'folks', u'follow', u'followers', u'food', u'football', u'force', u'forever', u'forget', u'forward', u'france', u'free', u'freedom', u'fresh', u'friday', u'friend', u'friends', u'fuck', u'fun', u'funny', u'future', u'gain', u'game', u'garden', u'gay', u'gbp', u'generation', u'germany', u'gift', u'girl', u'girls', u'glad', u'glass', u'goal', u'god', u'gonna', u'good', u'google', u'gop', u'gorgeous', u'gorilla', u'gotta', u'graduate', u'grateful', u'great', u'greatest', u'green', u'grimmie', u'group', u'grow', u'guess', u'gun', u'guy', u'gym', u'haha', u'hair', u'half', u'hand', u'hang', u'happen', u'happier', u'happiest', u'happiness', u'happy', u'hard', u'hardcore', u'hat', u'hate', u'hatred', u'hea', u'head', u'heal', u'health', u'healthy', u'hear', u'heas', u'hell', u'hello', u'help', u'hero', u'hey', u'high', u'hill', u'hillary', u'history', u'hit', u'hold', u'holiday', u'home', u'honor', u'hop', u'hope', u'hopefully', u'horrible', u'host', u'hot', u'hotel', u'hour', u'hours', u'house', u'hrs', u'htt', u'hug', u'huge', u'human', u'humanity', u'humans', u'husband', u'ice', u'idea', u'ignorance', u'ignore', u'image', u'imagine', u'impoant', u'include', u'india', u'info', u'injure', u'innocent', u'inside', u'inspire', u'instagram', u'instead', u'internet', u'interview', u'involve', u'islam', u'issue', u'jam', u'job', u'john', u'join', u'joke', u'journey', u'joy', u'jpy', u'judge', u'july', u'june', u'key', u'kick', u'kid', u'kill', u'kind', u'king', u'kiss', u'know', u'lack', u'ladies', u'lady', u'lake', u'land', u'late', u'later', u'latest', u'laugh', u'launch', u'law', u'lead', u'leakage', u'learn', u'leave', u'lebron', u'let', u'level', u'libtard', u'lie', u'life', u'light', u'like', u'line', u'link', u'list', u'listen', u'literally', u'little', u'live', u'local', u'logins', u'lol', u'london', u'long', u'longer', u'look', u'lord', u'lose', u'loss', u'lot', u'love', u'lovely', u'lover', u'low', u'luck', u'lucky', u'lunch', u'lyric', u'mad', u'magic', u'main', u'make', u'man', u'mark', u'market', u'marry', u'mass', u'match', u'material', u'matter', u'maybe', u'mean', u'media', u'meet', u'member', u'members', u'memories', u'men', u'mention', u'message', u'michael', u'middle', u'million', u'mind', u'mindset', u'mins', u'minute', u'minutes', u'miss', u'mix', u'mom', u'moment', u'moments', u'monday', u'money', u'month', u'months', u'mood', u'moon', u'morning', u'mother', u'mountains', u'movie', u'movies', u'murder', u'music', u'muslim', u'muslims', u'nail', u'naked', u'nation', u'national', u'nba', u'near', u'nearly', u'need', u'negative', u'ness', u'new', u'news', u'nice', u'niggas', u'night', u'nights', u'non', u'note', u'notice', u'nude', u'number', u'obama', u'offer', u'office', u'officer', u'official', u'officially', u'oil', u'okay', u'old', u'omg', u'ones', u'online', u'open', u'orange', u'order', u'organizations', u'orlando', u'outside', u'pack', u'page', u'pain', u'paint', u'parent', u'park', u'pass', u'past', u'pathetic', u'pay', u'peace', u'people', u'perfect', u'perform', u'performance', u'person', u'phone', u'photo', u'pic', u'pick', u'pics', u'picture', u'piece', u'pig', u'pizza', u'place', u'plan', u'play', u'pls', u'plus', u'poetry', u'point', u'polar', u'police', u'political', u'politics', u'poor', u'pop', u'porn', u'positive', u'possible', u'post', u'power', u'powerful', u'ppl', u'practice', u'pray', u'prayers', u'pre', u'prepare', u'present', u'president', u'press', u'pretty', u'previous', u'price', u'pride', u'print', u'probably', u'problem', u'problems', u'product', u'profile', u'project', u'promote', u'protest', u'proud', u'prove', u'public', u'purchase', u'pussy', u'queen', u'question', u'quick', u'quote', u'race', u'racism', u'racist', u'rain', u'raise', u'rally', u'ramadan', u'rat', u'reach', u'read', u'ready', u'real', u'reality', u'realize', u'really', u'reason', u'receive', u'record', u'red', u'register', u'relax', u'release', u'religion', u'remain', u'remember', u'remind', u'respect', u'response', u'rest', u'result', u'return', u'retweet', u'reveal', u'review', u'rid', u'ride', u'right', u'rip', u'rise', u'risk', u'road', u'robe', u'rock', u'role', u'roll', u'room', u'rooster', u'round', u'route', u'row', u'rule', u'run', u'sad', u'sadly', u'safe', u'sale', u'saturday', u'save', u'saw', u'say', u'scar', u'school', u'screen', u'sea', u'search', u'season', u'seat', u'second', u'secret', u'seek', u'self', u'selfie', u'sell', u'send', u'sense', u'senseless', u'series', u'seriously', u'serve', u'service', u'session', u'set', u'sex', u'sexy', u'shame', u'share', u'shi', u'shift', u'shit', u'sho', u'shock', u'shoe', u'shoot', u'shop', u'shout', u'shut', u'sick', u'sign', u'simple', u'simulation', u'simulator', u'sing', u'single', u'sister', u'sit', u'size', u'skin', u'sky', u'sleep', u'slow', u'small', u'smh', u'smile', u'snapchat', u'social', u'somebody', u'son', u'song', u'songs', u'soon', u'sorry', u'soul', u'sound', u'source', u'south', u'space', u'speak', u'special', u'speech', u'spend', u'spos', u'spread', u'sta', u'staed', u'staff', u'stage', u'staing', u'stand', u'star', u'stas', u'state', u'stay', u'steal', u'step', u'stick', u'stock', u'stomp', u'stop', u'store', u'stories', u'story', u'straight', u'stream', u'street', u'strong', u'students', u'study', u'stuff', u'stupid', u'style', u'success', u'suck', u'suffer', u'summer', u'sun', u'sunday', u'sunny', u'sunshine', u'super', u'suppo', u'suppoers', u'suppoing', u'suppose', u'sure', u'surprise', u'survive', u'sweet', u'swim', u'tag', u'talk', u'target', u'task', u'tea', u'teach', u'team', u'tear', u'teen', u'tell', u'term', u'terrible', u'terrorist', u'test', u'thank', u'thankful', u'thats', u'thing', u'things', u'think', u'tho', u'thoughts', u'throw', u'thursday', u'ticket', u'til', u'till', u'time', u'tip', u'tire', u'today', u'tomorrow', u'tonight', u'tool', u'totally', u'touch', u'tour', u'town', u'track', u'tragedy', u'tragic', u'train', u'travel', u'treat', u'tree', u'trend', u'trip', u'troll', u'true', u'truly', u'trump', u'trust', u'truth', u'try', u'tuesday', u'tune', u'turn', u'tweet', u'twitter', u'type', u'understand', u'unite', u'university', u'update', u'usa', u'usd', u'use', u'useful', u'vacation', u'value', u'vast', u'vegas', u'vibes', u'vicinity', u'victims', u'video', u'videos', u'view', u'vine', u'violence', u'visit', u'voice', u'vote', u'wait', u'wake', u'wakeup', u'walk', u'wall', u'wanna', u'want', u'war', u'warm', u'waste', u'watch', u'water', u'way', u'ways', u'wear', u'weather', u'web', u'website', u'wed', u'wednesday', u'week', u'weekend', u'weeks', u'welcome', u'west', u'white', u'wife', u'wild', u'win', u'wine', u'wing', u'winner', u'wish', u'woh', u'woman', u'women', u'wonder', u'wonderful', u'word', u'work', u'world', u'worry', u'worse', u'worst', u'wow', u'write', u'wrong', u'wtf', u'xxx', u'yay', u'yeah', u'year', u'years', u'yes', u'yesterday', u'york', u'young', u'youth', u'youtube', u'yrs']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero elements: 145459\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Non-zero elements: 244307\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "  3818 4384 2013 1755 2143 4700]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0  541 4251\n",
      "  1142 4685 1509 1445   39 2756]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0 3863   70]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0  856 4473 4203]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0  821 2670]]\n"
     ]
    }
   ],
   "source": [
    "# feature extraction\n",
    "# ------------------\n",
    "max_features = 1000\n",
    "\n",
    "# obtain Bag-of-Words (BoW), so it can be used to compute term frequencies --> generates a very large feature space, which must be kept small artificially\n",
    "#count_vectorizer = CountVectorizer()\n",
    "count_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=max_features, stop_words='english')\n",
    "\n",
    "count_vectorizer__train_test = count_vectorizer.fit_transform(df_train_test['tweet'])\n",
    "BoW_count__train_test = count_vectorizer.get_feature_names()\n",
    "termfreq__train_test = count_vectorizer__train_test.toarray()\n",
    "\n",
    "print BoW_count__train_test\n",
    "print termfreq__train_test\n",
    "print \"Non-zero elements:\", np.count_nonzero(termfreq__train_test)\n",
    "\n",
    "# compute tf-idf frequencies: reflects how important a word is to a document in a collection --> still generates a very large feature space, which must be kept small artificially\n",
    "#tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=max_features, stop_words='english')\n",
    "\n",
    "tfidf_vectorizer__train_test = tfidf_vectorizer.fit_transform(df_train_test['tweet'])\n",
    "BoW_tfidf__train_test = tfidf_vectorizer.get_feature_names()\n",
    "tfidf__train_test = tfidf_vectorizer__train_test.toarray()\n",
    "\n",
    "print BoW_tfidf__train_test\n",
    "print tfidf__train_test\n",
    "print \"Non-zero elements:\", np.count_nonzero(tfidf__train_test)\n",
    "\n",
    "# compute hashed frequencies: performs feature hashing ('hashing trick') to control the size of the feature space naturally\n",
    "hashfreq_vectorizer = HashingVectorizer(n_features=max_features)\n",
    "\n",
    "hashfreq_vectorizer__train_test = hashfreq_vectorizer.transform(df_train_test['tweet'])\n",
    "hashfreq__train_test = hashfreq_vectorizer__train_test.toarray()\n",
    "\n",
    "print hashfreq__train_test\n",
    "print \"Non-zero elements:\", np.count_nonzero(hashfreq__train_test)\n",
    "\n",
    "\n",
    "# compute N-grams (N=2): probabilistic language model for predicting the next word in a word sequence (ex: tweet)\n",
    "# ### NOT ENOUGH TIME TO IMPLEMENT ###\n",
    "\n",
    "\n",
    "# compute one-hot encoding on the tweets, as a preparation for computing word embeddings (using word2vec)\n",
    "#   integer encode the tweets\n",
    "vocab_size = 5000\n",
    "onehot__train_test = [one_hot(tweet, vocab_size) for tweet in df_train_test['tweet']]\n",
    "\n",
    "#   truncate and pad the tweets: normalization required for computing the word embeddings\n",
    "max_tweet_length = 20\n",
    "onehot__train_test = sequence.pad_sequences(onehot__train_test, maxlen=max_tweet_length)\n",
    "\n",
    "print onehot__train_test[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    29720\n",
      "1     2242\n",
      "Name: label, dtype: int64\n",
      "termfreq: Counter({0: 20815, 1: 1558}) Counter({0: 8905, 1: 684})\n",
      "tfidf: Counter({0: 20815, 1: 1558}) Counter({0: 8905, 1: 684})\n",
      "hashfreq: Counter({0: 20815, 1: 1558}) Counter({0: 8905, 1: 684})\n",
      "onehot: Counter({0: 20815, 1: 1558}) Counter({0: 8905, 1: 684})\n"
     ]
    }
   ],
   "source": [
    "# split training and testing feature samples, according to the original separation\n",
    "# --------------------------------------------------------------------------------\n",
    "termfreq_train = termfreq__train_test[:31962,:]\n",
    "termfreq_test = termfreq__train_test[31962:,:]\n",
    "\n",
    "tfidf_train = tfidf__train_test[:31962,:]\n",
    "tfidf_test = tfidf__train_test[31962:,:]\n",
    "\n",
    "hashfreq_train = hashfreq__train_test[:31962,:]\n",
    "hashfreq_test = hashfreq__train_test[31962:,:]\n",
    "\n",
    "onehot_train = onehot__train_test[:31962,:]\n",
    "onehot_test = onehot__train_test[31962:,:]\n",
    "\n",
    "# split training data into training and validation sets\n",
    "# -----------------------------------------------------\n",
    "x_termfreq_train, x_termfreq_valid, y_termfreq_train, y_termfreq_valid = train_test_split(termfreq_train, df_train_BASE['label'], random_state=42, test_size=0.3)\n",
    "x_tfidf_train, x_tfidf_valid, y_tfidf_train, y_tfidf_valid = train_test_split(tfidf_train, df_train_BASE['label'], random_state=42, test_size=0.3)\n",
    "x_hashfreq_train, x_hashfreq_valid, y_hashfreq_train, y_hashfreq_valid = train_test_split(hashfreq_train, df_train_BASE['label'], random_state=42, test_size=0.3)\n",
    "x_onehot_train, x_onehot_valid, y_onehot_train, y_onehot_valid = train_test_split(onehot_train, df_train_BASE['label'], random_state=42, test_size=0.3)\n",
    "\n",
    "print df_train_BASE['label'].value_counts()\n",
    "print \"termfreq:\", Counter(y_termfreq_train), Counter(y_termfreq_valid)\n",
    "print \"tfidf:\", Counter(y_tfidf_train), Counter(y_tfidf_valid)\n",
    "print \"hashfreq:\", Counter(y_hashfreq_train), Counter(y_hashfreq_valid)\n",
    "print \"onehot:\", Counter(y_onehot_train), Counter(y_onehot_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Logistic Regression ###\n",
      "   = term frequency =\n",
      "f-measure TRAIN: 0.5742251223491027\n",
      "f-measure VALID: 0.44981132075471697\n",
      "   = tfidf =\n",
      "f-measure TRAIN: 0.5533442088091354\n",
      "f-measure VALID: 0.463768115942029\n",
      "   = hashed frequencies =\n",
      "f-measure TRAIN: 0.5254569190600522\n",
      "f-measure VALID: 0.42997728993186973\n"
     ]
    }
   ],
   "source": [
    "# data modelling and evaluation - Logistic Regression\n",
    "# ---------------------------------------------------\n",
    "print \"### Logistic Regression ###\"\n",
    "\n",
    "def LogisticRegression_prediction(x_train, y_train, x_valid, y_valid, x_test, threshold):\n",
    "    logistic_regression = LogisticRegression()\n",
    "    \n",
    "    # training the model\n",
    "    logistic_regression.fit(x_train, y_train)\n",
    "    \n",
    "    # computing probability predictions\n",
    "    pred_train = logistic_regression.predict_proba(x_train)\n",
    "    pred_valid = logistic_regression.predict_proba(x_valid)\n",
    "    pred_test = logistic_regression.predict_proba(x_test)\n",
    "    \n",
    "    # compute binary predictions from probabilities\n",
    "    pred_int_train = (pred_train[:,1] >= threshold).astype(np.int)\n",
    "    pred_int_valid = (pred_valid[:,1] >= threshold).astype(np.int)\n",
    "    pred_int_test = (pred_test[:,1] >= threshold).astype(np.int)\n",
    "    \n",
    "    # compute f-measure\n",
    "    print \"f-measure TRAIN:\", f1_score(y_train, pred_int_train)\n",
    "    print \"f-measure VALID:\", f1_score(y_valid, pred_int_valid)\n",
    "    \n",
    "    return pred_int_test\n",
    "\n",
    "# parameters --> should be chosen through grid search\n",
    "threshold = 0.20\n",
    "\n",
    "print \"   = term frequency =\"\n",
    "(pred_LR_termfreq_test) = LogisticRegression_prediction(x_termfreq_train, y_termfreq_train, x_termfreq_valid, y_termfreq_valid, termfreq_test, threshold)\n",
    "\n",
    "print \"   = tfidf =\"\n",
    "(pred_LR_tfidf_test) = LogisticRegression_prediction(x_tfidf_train, y_tfidf_train, x_tfidf_valid, y_tfidf_valid, tfidf_test, threshold)\n",
    "\n",
    "print \"   = hashed frequencies =\"\n",
    "(pred_LR_hashfreq_test) = LogisticRegression_prediction(x_hashfreq_train, y_hashfreq_train, x_hashfreq_valid, y_hashfreq_valid, hashfreq_test, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Support Vector Machine ###\n",
      "   = term frequency =\n",
      "f-measure TRAIN: 0.5194805194805195\n",
      "f-measure VALID: 0.42923219241443106\n",
      "   = tfidf =\n",
      "f-measure TRAIN: 0.4782608695652173\n",
      "f-measure VALID: 0.41505791505791506\n",
      "   = hashed frequencies =\n",
      "f-measure TRAIN: 0.5292479108635098\n",
      "f-measure VALID: 0.3951048951048951\n"
     ]
    }
   ],
   "source": [
    "# data modelling and evaluation - Support Vector Machine\n",
    "# ------------------------------------------------------\n",
    "print \"### Support Vector Machine ###\"\n",
    "\n",
    "def SupportVectorMachine_prediction(x_train, y_train, x_valid, y_valid, x_test, C, threshold):\n",
    "    SVM = SVC(C=C, kernel='linear', probability=True)\n",
    "    \n",
    "    # training the model\n",
    "    SVM.fit(x_train, y_train);\n",
    "    \n",
    "    # computing probability predictions\n",
    "    pred_prob_train = SVM.predict_proba(x_train);\n",
    "    pred_prob_valid = SVM.predict_proba(x_valid);\n",
    "    pred_prob_test = SVM.predict_proba(x_test);\n",
    "    \n",
    "    # compute binary predictions from probabilities\n",
    "    pred_int_train = (pred_prob_train[:,1] >= threshold).astype(np.int)\n",
    "    pred_int_valid = (pred_prob_valid[:,1] >= threshold).astype(np.int)\n",
    "    pred_int_test = (pred_prob_test[:,1] >= threshold).astype(np.int)\n",
    "    \n",
    "    # compute f-measure\n",
    "    print \"f-measure TRAIN:\", f1_score(y_train, pred_int_train)\n",
    "    print \"f-measure VALID:\", f1_score(y_valid, pred_int_valid)\n",
    "    \n",
    "    return pred_int_test\n",
    "\n",
    "# parameters --> should be chosen through grid search\n",
    "C = 1\n",
    "threshold = 0.2\n",
    "\n",
    "# ### DO NOT EXECUTE, IT TAKES A VERY LONG TIME TO RUN <-- TOO MANY FEATURES ###\n",
    "\n",
    "print \"   = term frequency =\"\n",
    "(pred_SVM_termfreq_test) = SupportVectorMachine_prediction(x_termfreq_train, y_termfreq_train, x_termfreq_valid, y_termfreq_valid, termfreq_test, C, threshold)\n",
    "\n",
    "print \"   = tfidf =\"\n",
    "(pred_SVM_tfidf_test) = SupportVectorMachine_prediction(x_tfidf_train, y_tfidf_train, x_tfidf_valid, y_tfidf_valid, tfidf_test, C, threshold)\n",
    "\n",
    "print \"   = hashed frequencies =\"\n",
    "(pred_SVM_hashfreq_test) = SupportVectorMachine_prediction(x_hashfreq_train, y_hashfreq_train, x_hashfreq_valid, y_hashfreq_valid, hashfreq_test, C, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Decision Tree ###\n",
      "   = term frequency =\n",
      "f-measure TRAIN: 0.2980030721966206\n",
      "f-measure VALID: 0.2942528735632184\n",
      "   = tfidf =\n",
      "f-measure TRAIN: 0.33984375\n",
      "f-measure VALID: 0.3179255918827509\n",
      "   = hashed frequencies =\n",
      "f-measure TRAIN: 0.2679671457905544\n",
      "f-measure VALID: 0.24470588235294116\n"
     ]
    }
   ],
   "source": [
    "# data modelling and evaluation - Decision Tree\n",
    "# ---------------------------------------------\n",
    "print \"### Decision Tree ###\"\n",
    "\n",
    "def DecisionTree_prediction(x_train, y_train, x_valid, y_valid, x_test, max_depth, min_samples_leaf):\n",
    "    decision_tree = DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
    "    \n",
    "    # training the model\n",
    "    decision_tree.fit(x_train, y_train);\n",
    "    \n",
    "    # compute binary predictions\n",
    "    pred_train = decision_tree.predict(x_train);\n",
    "    pred_valid = decision_tree.predict(x_valid);\n",
    "    pred_test = decision_tree.predict(x_test);\n",
    "    \n",
    "    # compute f-measure\n",
    "    print \"f-measure TRAIN:\", f1_score(y_train, pred_train)\n",
    "    print \"f-measure VALID:\", f1_score(y_valid, pred_valid)\n",
    "    \n",
    "    return pred_test\n",
    "\n",
    "# parameters --> should be chosen through grid search\n",
    "max_depth = 1000\n",
    "min_samples_leaf = 50\n",
    "\n",
    "print \"   = term frequency =\"\n",
    "(pred_DT_termfreq_test) = DecisionTree_prediction(x_termfreq_train, y_termfreq_train, x_termfreq_valid, y_termfreq_valid, termfreq_test, max_depth, min_samples_leaf)\n",
    "\n",
    "print \"   = tfidf =\"\n",
    "(pred_DT_tfidf_test) = DecisionTree_prediction(x_tfidf_train, y_tfidf_train, x_tfidf_valid, y_tfidf_valid, tfidf_test, max_depth, min_samples_leaf)\n",
    "\n",
    "print \"   = hashed frequencies =\"\n",
    "(pred_DT_hashfreq_test) = DecisionTree_prediction(x_hashfreq_train, y_hashfreq_train, x_hashfreq_valid, y_hashfreq_valid, hashfreq_test, max_depth, min_samples_leaf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Naive Bayes ###\n",
      "   = term frequency =\n",
      "f-measure TRAIN: 0.19450686641697876\n",
      "f-measure VALID: 0.17882836587872558\n",
      "   = tfidf =\n",
      "f-measure TRAIN: 0.201019289078124\n",
      "f-measure VALID: 0.1833358628016391\n",
      "   = hashed frequencies =\n",
      "f-measure TRAIN: 0.17500862961684502\n",
      "f-measure VALID: 0.15498056039683603\n"
     ]
    }
   ],
   "source": [
    "# data modelling and evaluation - Naive Bayes\n",
    "# -------------------------------------------\n",
    "print \"### Naive Bayes ###\"\n",
    "\n",
    "def NaiveBayes_prediction(x_train, y_train, x_valid, y_valid, x_test, threshold):\n",
    "    naive_bayes = GaussianNB()\n",
    "    \n",
    "    # training the model\n",
    "    naive_bayes.fit(x_train, y_train);\n",
    "    \n",
    "    # compute probability predictions\n",
    "    pred_train = naive_bayes.predict(x_train);\n",
    "    pred_valid = naive_bayes.predict(x_valid);\n",
    "    pred_test = naive_bayes.predict(x_test);\n",
    "    \n",
    "    # compute f-measure\n",
    "    print \"f-measure TRAIN:\", f1_score(y_train, pred_train)\n",
    "    print \"f-measure VALID:\", f1_score(y_valid, pred_valid)\n",
    "    \n",
    "    return pred_test\n",
    "\n",
    "# parameters --> should be chosen through grid search\n",
    "threshold = 0.2\n",
    "\n",
    "print \"   = term frequency =\"\n",
    "(pred_DT_termfreq_test) = NaiveBayes_prediction(x_termfreq_train, y_termfreq_train, x_termfreq_valid, y_termfreq_valid, termfreq_test, threshold)\n",
    "\n",
    "print \"   = tfidf =\"\n",
    "(pred_DT_tfidf_test) = NaiveBayes_prediction(x_tfidf_train, y_tfidf_train, x_tfidf_valid, y_tfidf_valid, tfidf_test, threshold)\n",
    "\n",
    "print \"   = hashed frequencies =\"\n",
    "(pred_DT_hashfreq_test) = NaiveBayes_prediction(x_hashfreq_train, y_hashfreq_train, x_hashfreq_valid, y_hashfreq_valid, hashfreq_test, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define metric 'F-measure' (no longer available in Keras) for evaluating Deep Learning models\n",
    "def fmeasure(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        \n",
    "        recall = true_positives / (possible_positives + K.epsilon()) # epsilon: avoids 'division by 0'\n",
    "        \n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        \n",
    "        precision = true_positives / (predicted_positives + K.epsilon()) # epsilon: avoids 'division by 0'\n",
    "        \n",
    "        return precision\n",
    "    \n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    \n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon())) # epsilon: avoids 'division by 0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                64064     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 64,129\n",
      "Trainable params: 64,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 22373 samples, validate on 9589 samples\n",
      "Epoch 1/10\n",
      "22373/22373 [==============================] - 3s 133us/step - loss: 0.2856 - fmeasure: 0.0029 - acc: 0.9276 - val_loss: 0.2209 - val_fmeasure: 0.0653 - val_acc: 0.9315\n",
      "Epoch 2/10\n",
      "22373/22373 [==============================] - 3s 124us/step - loss: 0.1958 - fmeasure: 0.2186 - acc: 0.9387 - val_loss: 0.2147 - val_fmeasure: 0.2350 - val_acc: 0.9379\n",
      "Epoch 3/10\n",
      "22373/22373 [==============================] - 3s 122us/step - loss: 0.1856 - fmeasure: 0.3183 - acc: 0.9430 - val_loss: 0.2106 - val_fmeasure: 0.2772 - val_acc: 0.9398\n",
      "Epoch 4/10\n",
      "22373/22373 [==============================] - 3s 125us/step - loss: 0.1796 - fmeasure: 0.3582 - acc: 0.9451 - val_loss: 0.2082 - val_fmeasure: 0.3078 - val_acc: 0.9407\n",
      "Epoch 5/10\n",
      "22373/22373 [==============================] - 3s 123us/step - loss: 0.1750 - fmeasure: 0.3754 - acc: 0.9461 - val_loss: 0.2097 - val_fmeasure: 0.3057 - val_acc: 0.9411\n",
      "Epoch 6/10\n",
      "22373/22373 [==============================] - 3s 124us/step - loss: 0.1726 - fmeasure: 0.4020 - acc: 0.9468 - val_loss: 0.2050 - val_fmeasure: 0.3283 - val_acc: 0.9416\n",
      "Epoch 7/10\n",
      "22373/22373 [==============================] - 3s 130us/step - loss: 0.1697 - fmeasure: 0.3950 - acc: 0.9475 - val_loss: 0.2030 - val_fmeasure: 0.3442 - val_acc: 0.9422\n",
      "Epoch 8/10\n",
      "22373/22373 [==============================] - 3s 129us/step - loss: 0.1673 - fmeasure: 0.4024 - acc: 0.9477 - val_loss: 0.2043 - val_fmeasure: 0.3317 - val_acc: 0.9418\n",
      "Epoch 9/10\n",
      "22373/22373 [==============================] - 3s 124us/step - loss: 0.1650 - fmeasure: 0.4262 - acc: 0.9486 - val_loss: 0.2034 - val_fmeasure: 0.3433 - val_acc: 0.9418\n",
      "Epoch 10/10\n",
      "22373/22373 [==============================] - 3s 125us/step - loss: 0.1632 - fmeasure: 0.4381 - acc: 0.9494 - val_loss: 0.2019 - val_fmeasure: 0.3465 - val_acc: 0.9411\n",
      "f-measure TRAIN: 0.3745053441220901\n",
      "f-measure VALID: 0.2888379033830917\n"
     ]
    }
   ],
   "source": [
    "# data modelling and evaluation - MLP + term frequencies\n",
    "# ------------------------------------------------------\n",
    "# build the model\n",
    "num_units = 64\n",
    "\n",
    "MLP_model = Sequential()\n",
    "\n",
    "MLP_model.add(Dense(num_units, input_dim=max_features, activation='relu'))\n",
    "MLP_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "MLP_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=[fmeasure, 'accuracy'])\n",
    "\n",
    "print MLP_model.summary()\n",
    "\n",
    "# train the model\n",
    "MLP_model.fit(x_termfreq_train, y_termfreq_train, validation_data=(x_termfreq_valid, y_termfreq_valid), epochs=10, batch_size=64)\n",
    "\n",
    "# evaluate the model\n",
    "metrics_MLP_train = MLP_model.evaluate(x_termfreq_train, y_termfreq_train, verbose=0)\n",
    "metrics_MLP_valid = MLP_model.evaluate(x_termfreq_valid, y_termfreq_valid, verbose=0)\n",
    "\n",
    "print \"f-measure TRAIN:\", metrics_MLP_train[1]\n",
    "print \"f-measure VALID:\", metrics_MLP_valid[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 20, 8)             40000     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 5)                 280       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 40,286\n",
      "Trainable params: 40,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 22373 samples, validate on 9589 samples\n",
      "Epoch 1/10\n",
      "22373/22373 [==============================] - 14s 637us/step - loss: 0.3433 - fmeasure: 0.0024 - acc: 0.9249 - val_loss: 0.2458 - val_fmeasure: 0.0000e+00 - val_acc: 0.9287\n",
      "Epoch 2/10\n",
      "22373/22373 [==============================] - 13s 575us/step - loss: 0.2047 - fmeasure: 0.0095 - acc: 0.9308 - val_loss: 0.1913 - val_fmeasure: 0.0623 - val_acc: 0.9311\n",
      "Epoch 3/10\n",
      "22373/22373 [==============================] - 13s 571us/step - loss: 0.1465 - fmeasure: 0.3411 - acc: 0.9457 - val_loss: 0.1816 - val_fmeasure: 0.3458 - val_acc: 0.9418\n",
      "Epoch 4/10\n",
      "22373/22373 [==============================] - 13s 588us/step - loss: 0.1210 - fmeasure: 0.5545 - acc: 0.9583 - val_loss: 0.1845 - val_fmeasure: 0.4464 - val_acc: 0.9422\n",
      "Epoch 5/10\n",
      "22373/22373 [==============================] - 13s 573us/step - loss: 0.1045 - fmeasure: 0.6762 - acc: 0.9647 - val_loss: 0.1945 - val_fmeasure: 0.4028 - val_acc: 0.9433\n",
      "Epoch 6/10\n",
      "22373/22373 [==============================] - 13s 578us/step - loss: 0.0913 - fmeasure: 0.7367 - acc: 0.9697 - val_loss: 0.1987 - val_fmeasure: 0.4592 - val_acc: 0.9443\n",
      "Epoch 7/10\n",
      "22373/22373 [==============================] - 13s 568us/step - loss: 0.0816 - fmeasure: 0.7774 - acc: 0.9747 - val_loss: 0.2053 - val_fmeasure: 0.4565 - val_acc: 0.9367\n",
      "Epoch 8/10\n",
      "22373/22373 [==============================] - 13s 572us/step - loss: 0.0748 - fmeasure: 0.8095 - acc: 0.9770 - val_loss: 0.2108 - val_fmeasure: 0.4660 - val_acc: 0.9405\n",
      "Epoch 9/10\n",
      "22373/22373 [==============================] - 13s 574us/step - loss: 0.0689 - fmeasure: 0.8143 - acc: 0.9792 - val_loss: 0.2179 - val_fmeasure: 0.4597 - val_acc: 0.9403\n",
      "Epoch 10/10\n",
      "22373/22373 [==============================] - 13s 572us/step - loss: 0.0630 - fmeasure: 0.8377 - acc: 0.9818 - val_loss: 0.2252 - val_fmeasure: 0.4688 - val_acc: 0.9372\n",
      "f-measure TRAIN: 0.7783411642124329\n",
      "f-measure VALID: 0.4193384347761096\n"
     ]
    }
   ],
   "source": [
    "# data modelling and evaluation - LSTM + word2vec\n",
    "# -----------------------------------------------\n",
    "# build the model\n",
    "embedding_vector_length = 8\n",
    "num_memory_units = 5\n",
    "\n",
    "LSTM_model = Sequential()\n",
    "\n",
    "LSTM_model.add(Embedding(vocab_size, embedding_vector_length, input_length=max_tweet_length)) # word embedding layer\n",
    "LSTM_model.add(LSTM(num_memory_units)) # LSTM layer\n",
    "LSTM_model.add(Dense(1, activation='sigmoid')) # output layer\n",
    "\n",
    "LSTM_model.compile(loss='binary_crossentropy',optimizer='adam', metrics=[fmeasure, 'accuracy'])\n",
    "\n",
    "print LSTM_model.summary()\n",
    "\n",
    "# train the model\n",
    "LSTM_model.fit(x_onehot_train, y_onehot_train, validation_data=(x_onehot_valid, y_onehot_valid), epochs=10, batch_size=64)\n",
    "\n",
    "# evaluate the model\n",
    "metrics_LSTM_train = LSTM_model.evaluate(x_onehot_train, y_onehot_train, verbose=0) \n",
    "metrics_LSTM_valid = LSTM_model.evaluate(x_onehot_valid, y_onehot_valid, verbose=0) \n",
    "\n",
    "print \"f-measure TRAIN:\", metrics_LSTM_train[1]\n",
    "print \"f-measure VALID:\", metrics_LSTM_valid[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
